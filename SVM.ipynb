{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "THETA = 0.01\n",
    "LEARNING_RATE=0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # load CIFAR-10 train data\n",
    "    xTrain, yTrain = load_train_data(\"CIFAR-10-train\")\n",
    "\n",
    "    # load CIFAR-10 test data\n",
    "    xTest, yTest = load_test_data(\"CIFAR-10-test\")\n",
    "\n",
    "    # initialize a random weight matrix for CIFAR-10\n",
    "    W = np.random.rand(10, 3073)*0.001\n",
    "\n",
    "    eval_gradient(loss_fun,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only used in load_train_data() and load_test_data() to read pickle file\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(filename):\n",
    "    \"\"\"\n",
    "    Load CIFAR-10 train data\n",
    "    xTrain combined image arrays\n",
    "    yTrain is the vector of correct labels for each image\n",
    "    \"\"\"\n",
    "\n",
    "    # get train data 1-6 append them to a list\n",
    "    temp_xTrain = []\n",
    "    temp_yTrain = []\n",
    "    for file in os.listdir(filename):\n",
    "        data = unpickle(os.path.join(filename, file))\n",
    "        temp_xTrain.append(data[b'data'])\n",
    "        temp_yTrain.append(data[b'labels'])\n",
    "\n",
    "    # make one list from the list of lists train data is now ready\n",
    "    xTrain = np.concatenate(temp_xTrain)\n",
    "    yTrain = np.concatenate(temp_yTrain)\n",
    "\n",
    "    # append bias to images arrays\n",
    "    xTrain = np.c_[xTrain, np.ones(50000).T]\n",
    "\n",
    "    return (xTrain.T, yTrain.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(filename):\n",
    "    \"\"\"\n",
    "    Load CIFAR-10 test data\n",
    "    \"\"\"\n",
    "    data = unpickle(os.path.join(filename, os.listdir(filename)[0]))\n",
    "    xTest = (data[b'data'])\n",
    "    yTest = (data[b'labels'])\n",
    "    yTest = np.array(yTest)\n",
    "\n",
    "    # append bias to images arrays\n",
    "    xTest = np.c_[xTest, np.ones(10000).T]\n",
    "\n",
    "    return (xTest.T, yTest.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_loss(X, y, W, theta):\n",
    "    \"\"\"\n",
    "    Multiclass Support Vector Machine Loss\n",
    "\n",
    "    x is matrix of flattened image vectors with appended biases\n",
    "    y is the integer valued matrix representing correct labels for images\n",
    "    W is the weight matrix\n",
    "    theta is the hyperparameter and usually determined by cross-validation\n",
    "\n",
    "    We set up SVM loss so that for each image,\n",
    "    its score of correct class is higher than every other\n",
    "    incorrect class by a fixed margin.\n",
    "\n",
    "    Multiclass SVM loss is formalized as:\n",
    "\n",
    "    L = data_loss + theta*regularization_penalty\n",
    "\n",
    "    wherein \n",
    "    data_loss =  (1/numOfTrainingData) *   sum    (max(0, (s_j - s_yi + margin)))\n",
    "                                         (j != yi)\n",
    "    (we assign margin value 1 and reduce number of hyperparameters\n",
    "    because theta and margin are inverse proportional)\n",
    "\n",
    "    regularization penalty = sum (sum ((W_k,l)**2))\n",
    "                             (k)   (l)    \n",
    "    \"\"\"\n",
    "    margin = 1\n",
    "    score_matrix = W@X\n",
    "    yi_scores = score_matrix[y, np.arange(score_matrix.shape[1])]\n",
    "    yi_scores = np.matrix(yi_scores).T\n",
    "    margins = np.maximum(0, score_matrix - yi_scores + margin)\n",
    "    margins[y, np.arange(X.shape[1])] = 0\n",
    "    data_loss = np.mean(np.sum(margins, axis=0))\n",
    "    regularization_penalty = np.sum(np.square(W))\n",
    "\n",
    "    return data_loss + theta*regularization_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(W):\n",
    "    return SVM_loss(xTrain, yTrain, W, theta=THETA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_gradient(f,x):\n",
    "    \"\"\"\n",
    "    Analytic gradient\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3073, 10000)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19f8f9ebccd493d1979261b88c51ecd06bf2efdee26e5f5e5ddb3d1c8ea2e26f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
