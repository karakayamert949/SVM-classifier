{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------Train results:\n",
    "-----Mini-batch size 256\n",
    "---Fixed: EPOCHS = 1000,LEARNING_RATE = 1e-6\n",
    "\n",
    "THETA = 1e-5\n",
    "In epoch 1000, loss is 5.291985059220746\n",
    "In epoch 1000 accuracy is 0.346075\n",
    "\n",
    "THETA = 1e-4\n",
    "In epoch 1000, loss is 5.292198365836736\n",
    "In epoch 1000 accuracy is 0.348175\n",
    "\n",
    "THETA = 1e-3\n",
    "In epoch 1000, loss is 5.296268629829621\n",
    "In epoch 1000 accuracy is 0.34815\n",
    "\n",
    "THETA = 1e-2\n",
    "In epoch 1000, loss is 5.297345092785761\n",
    "In epoch 1000 accuracy is 0.347775\n",
    "\n",
    "\n",
    "\n",
    "---Fixed: EPOCHS = 1000,LEARNING_RATE = 8e-5\n",
    "THETA = 1e-3\n",
    "In epoch 1000, loss is 6.837051169043007\n",
    "In epoch 1000 accuracy is 0.413175\n",
    "\n",
    "\n",
    "\n",
    "---Fixed: EPOCHS = 1000,LEARNING_RATE = 1e-5\n",
    "\n",
    "THETA = 1e-5\n",
    "In epoch 1000, loss is 4.473005846222188\n",
    "In epoch 1000 accuracy is 0.387375\n",
    "\n",
    "THETA = 1e-4\n",
    "In epoch 1000, loss is 4.4734887011987485\n",
    "In epoch 1000 accuracy is 0.3884\n",
    "\n",
    "THETA = 1e-3\n",
    "In epoch 1000, loss is 4.4808710837865116\n",
    "In epoch 1000 accuracy is 0.385575\n",
    "\n",
    "THETA = 1e-2\n",
    "In epoch 1000, loss is 4.501979782009769\n",
    "In epoch 1000 accuracy is 0.38755\n",
    "\n",
    "\n",
    "\n",
    "---Fixed: EPOCHS = 1000,LEARNING_RATE = 1e-4\n",
    "\n",
    "THETA = 1e-5\n",
    "In epoch 1000, loss is 4.195538879880822\n",
    "In epoch 1000 accuracy is 0.401625\n",
    "\n",
    "THETA = 1e-4\n",
    "In epoch 1000, loss is 4.118187725967621\n",
    "In epoch 1000 accuracy is 0.4106\n",
    "\n",
    "THETA = 1e-3\n",
    "In epoch 1000, loss is 4.187295139050325\n",
    "In epoch 1000 accuracy is 0.40025\n",
    "\n",
    "THETA = 1e-2\n",
    "In epoch 1000, loss is 4.246982001690002\n",
    "In epoch 1000 accuracy is 0.41245\n",
    "\n",
    "\n",
    "\n",
    "---Fixed: EPOCHS = 1000,LEARNING_RATE = 1e-3\n",
    "\n",
    "Too much unstable\n",
    "THETA = 1e-5\n",
    "In epoch 1000, loss is 8.297607196745993\n",
    "In epoch 1000 accuracy is 0.329675\n",
    "\n",
    "---Fixed: EPOCHS = 1000,LEARNING_RATE = 7e-5\n",
    "THETA = 1e-4\n",
    "In epoch 1000, loss is 5.174179245491815\n",
    "In epoch 1000 accuracy is 0.412975\n",
    "\n",
    "-----Mini-batch size 512\n",
    "---Fixed: EPOCHS = 1000,LEARNING_RATE = 1e-4\n",
    "\n",
    "THETA = 1e-5\n",
    "In epoch 1000, loss is 4.077958433569045\n",
    "In epoch 1000 accuracy is 0.40925\n",
    "\n",
    "THETA = 1e-2\n",
    "In epoch 1000, loss is 4.62524982252704\n",
    "In epoch 1000 accuracy is 0.393\n",
    "\n",
    "-----Mini-batch size 128\n",
    "---Fixed: EPOCHS = 1000,LEARNING_RATE = 5e-5\n",
    "\n",
    "THETA = 1e-4\n",
    "In epoch 1000, loss is 4.328775130110178\n",
    "In epoch 1000 accuracy is 0.393875\n",
    "\n",
    "-------Validation Results:\n",
    "-----Mini-batch size 256\n",
    "THETA = 1e-4\n",
    "LEARNING_RATE = 7e-5\n",
    "EPOCHS = 1000\n",
    "Loss is 5.305431801173743,Accuracy is 0.3993\n",
    "\n",
    "THETA = 1e-3\n",
    "LEARNING_RATE = 8e-5\n",
    "EPOCHS = 1000\n",
    "Loss is 6.92635428403829,Accuracy is 0.4024\n",
    "\n",
    "-------Test Results:\n",
    "-----Mini-batch size 256\n",
    "THETA = 1e-3\n",
    "LEARNING_RATE = 8e-5\n",
    "EPOCHS = 1000\n",
    "Loss is 6.95635111972989,Accuracy is 0.3975\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "THETA = 1e-3\n",
    "LEARNING_RATE = 8e-5\n",
    "EPOCHS = 1000\n",
    "MARGIN = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainMain():\n",
    "    \"\"\"\n",
    "    Main function for training and getting weight matrix \n",
    "    \"\"\"\n",
    "    # load CIFAR-10 train, validation data\n",
    "    xTrain, yTrain, xValid, yValid = load_train_data(\"CIFAR-10-train\")\n",
    "\n",
    "    # load CIFAR-10 test data\n",
    "    xTest, yTest = load_test_data(\"CIFAR-10-test\")\n",
    "\n",
    "    # initialize a random weight matrix for CIFAR-10\n",
    "    W = np.random.rand(10, 3073)*0.001\n",
    "\n",
    "    pltEpoch=range(EPOCHS)\n",
    "    pltAccuracy=[]\n",
    "    pltLoss=[]\n",
    "\n",
    "    # train the weights, epoch loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss, accuracy = SVM_loss(xTrain, yTrain, W, theta=THETA)\n",
    "        print(\n",
    "            f'In epoch {epoch+1}, loss is {loss}\\nIn epoch {epoch+1} accuracy is {accuracy}')\n",
    "\n",
    "        pltAccuracy.append(accuracy)\n",
    "        pltLoss.append(loss)\n",
    "\n",
    "        # mini-batch sample 256 examples\n",
    "        data_batch, yData_batch = sample_training_data(xTrain, yTrain, 256)\n",
    "\n",
    "        weights_grad = evaluate_gradient(data_batch, yData_batch, W)\n",
    "\n",
    "        # perform parameter update\n",
    "        W -= LEARNING_RATE * weights_grad\n",
    "\n",
    "    plotResults(pltEpoch,pltAccuracy,pltLoss)\n",
    "    np.savetxt('W.txt', W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValidationMain():\n",
    "    \"\"\"\n",
    "    Main function for validating the Validation set\n",
    "    \"\"\"\n",
    "    # load CIFAR-10 train, validation data\n",
    "    xTrain, yTrain, xValid, yValid = load_train_data(\"CIFAR-10-train\")\n",
    "\n",
    "    # load CIFAR-10 test data\n",
    "    xTest, yTest = load_test_data(\"CIFAR-10-test\")\n",
    "\n",
    "    # get W from Trainmain() use it to validate\n",
    "    W = np.loadtxt('W.txt')\n",
    "\n",
    "    # calculate loss and accuracy\n",
    "    loss, accuracy = SVM_loss(xValid, yValid, W, theta=THETA)\n",
    "    print(\n",
    "        f'Loss is {loss},Accuracy is {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestMain():\n",
    "    \"\"\"\n",
    "    Main function for testing the Test set\n",
    "    \"\"\"\n",
    "    # load CIFAR-10 train, validation data\n",
    "    xTrain, yTrain, xValid, yValid = load_train_data(\"CIFAR-10-train\")\n",
    "\n",
    "    # load CIFAR-10 test data\n",
    "    xTest, yTest = load_test_data(\"CIFAR-10-test\")\n",
    "\n",
    "    # get W from Trainmain() use it to test\n",
    "    W = np.loadtxt('W.txt')\n",
    "\n",
    "    # calculate loss and accuracy\n",
    "    loss, accuracy = SVM_loss(xTest, yTest, W, theta=THETA)\n",
    "    print(\n",
    "        f'Loss is {loss},Accuracy is {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    # only used in load_train_data() and load_test_data() to read pickle file\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(filename):\n",
    "    \"\"\"\n",
    "    Load CIFAR-10 train data\n",
    "    xTrain combined image arrays\n",
    "    yTrain is the vector of correct labels for each image\n",
    "    \"\"\"\n",
    "\n",
    "    # get train data 1-6 append them to a list\n",
    "    temp_xTrain = []\n",
    "    temp_yTrain = []\n",
    "    for file in os.listdir(filename):\n",
    "        data = unpickle(os.path.join(filename, file))\n",
    "        temp_xTrain.append(data[b'data'])\n",
    "        temp_yTrain.append(data[b'labels'])\n",
    "\n",
    "    # make one list from the list of lists train data is now ready\n",
    "    xTrain = np.concatenate(temp_xTrain)\n",
    "    yTrain = np.concatenate(temp_yTrain)\n",
    "\n",
    "    # change datatype to float\n",
    "    xTrain = xTrain.astype('float64')\n",
    "\n",
    "    # normalize data ???\n",
    "    for id, im in enumerate(xTrain):\n",
    "        normIm = im/255\n",
    "        xTrain[id] = normIm\n",
    "\n",
    "    # center data ???\n",
    "    avIm = xTrain.mean(axis=0, keepdims=True)\n",
    "    for id, im in enumerate(xTrain):\n",
    "        normIm = im-avIm\n",
    "        xTrain[id] = normIm\n",
    "\n",
    "    # append bias to images arrays\n",
    "    xTrain = np.c_[xTrain, np.ones(50000).T]\n",
    "\n",
    "    # assign 1/5 of the images as validation dataset\n",
    "    xValid = xTrain[:len(xTrain)//5]\n",
    "    xTrain = xTrain[len(xTrain)//5:]\n",
    "\n",
    "    yValid = yTrain[:len(yTrain)//5]\n",
    "    yTrain = yTrain[len(yTrain)//5:]\n",
    "\n",
    "    # transpose so xTrain is 3073x40000 yTrain is 40000x1 xValid is 3073x10000 yValid is 10000x1\n",
    "    xTrain, yTrain, xValid, yValid = xTrain.T, yTrain.T, xValid.T, yValid.T\n",
    "\n",
    "    return (xTrain, yTrain, xValid, yValid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(filename):\n",
    "    \"\"\"\n",
    "    Load CIFAR-10 test data\n",
    "    \"\"\"\n",
    "    data = unpickle(os.path.join(filename, os.listdir(filename)[0]))\n",
    "    xTest = (data[b'data'])\n",
    "    yTest = (data[b'labels'])\n",
    "    yTest = np.array(yTest)\n",
    "\n",
    "    # change datatype to float\n",
    "    xTest = xTest.astype('float64')\n",
    "\n",
    "    # normalize data ???\n",
    "    for id, im in enumerate(xTest):\n",
    "        normIm = im/255\n",
    "        xTest[id] = normIm\n",
    "\n",
    "    # center data ???\n",
    "    avIm = xTest.mean(axis=0, keepdims=True)\n",
    "    for id, im in enumerate(xTest):\n",
    "        normIm = im-avIm\n",
    "        xTest[id] = normIm\n",
    "\n",
    "    # append bias to images arrays\n",
    "    xTest = np.c_[xTest, np.ones(10000).T]\n",
    "\n",
    "    # transpose so that xTest is 3073x10000 yTest is 10000\n",
    "    xTest, yTest = xTest.T, yTest.T\n",
    "    return (xTest, yTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_loss(X, y, W, theta):\n",
    "    \"\"\"\n",
    "    Multiclass Support Vector Machine Loss\n",
    "\n",
    "    x is matrix of flattened image vectors with appended biases\n",
    "    y is the integer valued matrix representing correct labels for images\n",
    "    W is the weight matrix\n",
    "    theta is the hyperparameter and usually determined by cross-validation\n",
    "\n",
    "    We set up SVM loss so that for each image,\n",
    "    its score of correct class is higher than every other\n",
    "    incorrect class by a fixed margin.\n",
    "\n",
    "    Multiclass SVM loss is formalized as:\n",
    "\n",
    "    L = data_loss + theta*regularization_penalty\n",
    "\n",
    "    wherein \n",
    "    data_loss =  (1/numOfTrainingData) *   sum    (max(0, (s_j - s_yi + margin)))\n",
    "                                         (j != yi)\n",
    "    (we assign margin value 1 and reduce number of hyperparameters\n",
    "    because theta and margin are inverse proportional)\n",
    "\n",
    "    regularization penalty = sum (sum ((W_k,l)**2))\n",
    "                             (k)   (l)    \n",
    "    \"\"\"\n",
    "    # weight times images\n",
    "    score_matrix = W@X\n",
    "\n",
    "    # get the correct class score for every image\n",
    "    yi_scores = score_matrix[y, np.arange(score_matrix.shape[1])]\n",
    "    yi_scores = np.matrix(yi_scores)\n",
    "\n",
    "    # margins equals to below part, to ignore when s_j == s_yi we overwrite them by assigning 0s\n",
    "    #    sum    (max(0, (s_j - s_yi + margin)))\n",
    "    # (j != yi)\n",
    "    margins = np.maximum(0, score_matrix - yi_scores + MARGIN)\n",
    "    margins[y, np.arange(X.shape[1])] = 0\n",
    "\n",
    "    # data loss and regularization part\n",
    "    data_loss = np.mean(np.sum(margins, axis=0))\n",
    "    regularization_penalty = np.sum(np.square(W))\n",
    "\n",
    "    # total loss\n",
    "    SVM_loss_score = data_loss + theta*regularization_penalty\n",
    "\n",
    "    # find class where the score is the highest\n",
    "    yPredict = np.argmax(score_matrix, axis=0)\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = np.mean(yPredict == y)\n",
    "\n",
    "    return (SVM_loss_score, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_training_data(trainData, yTrainData, num):\n",
    "    \"\"\"\n",
    "    Mini-batch \n",
    "    num is the sample size\n",
    "    \"\"\"\n",
    "    randIndices = np.random.choice(trainData.shape[1], num, replace=False)\n",
    "    return (trainData[:, randIndices], yTrainData[randIndices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gradient(data_batch, yData_batch, W):\n",
    "  \"\"\"\n",
    "  Gradient of SVM loss function\n",
    "  data_batch consists random images selected from training set \n",
    "  yData_batch consists correct labels for each image\n",
    "  \"\"\"\n",
    "  num_train=data_batch.shape[1]\n",
    "  num_classes=W.shape[0]\n",
    "  weight_grads=np.zeros(W.shape)\n",
    "\n",
    "  for imgId in range(num_train):\n",
    "      scores = W@data_batch[:,imgId]\n",
    "      correct_class_score = scores[yData_batch[imgId]]\n",
    "      for clsid in range(num_classes):\n",
    "        margin = scores[clsid] - correct_class_score + MARGIN\n",
    "        if margin > 0:\n",
    "          if clsid == yData_batch[imgId]:\n",
    "            weight_grads[clsid] -= data_batch[:,imgId] \n",
    "          else:\n",
    "            weight_grads[clsid] += data_batch[:,imgId]\n",
    "  return weight_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResults(pltEpoch,pltAccuracy,pltLoss):\n",
    "    \"\"\"\n",
    "    Plot accuracy-epoch and loss-epoch\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(pltEpoch,pltAccuracy)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Epoch')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(pltEpoch,pltLoss)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #TrainMain()\n",
    "    #ValidationMain()\n",
    "    #TestMain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19f8f9ebccd493d1979261b88c51ecd06bf2efdee26e5f5e5ddb3d1c8ea2e26f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
