{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "THETA = 0.01\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # load CIFAR-10 train, validation data\n",
    "    xTrain, yTrain, xValid, yValid = load_train_data(\"CIFAR-10-train\")\n",
    "\n",
    "    # load CIFAR-10 test data\n",
    "    xTest, yTest = load_test_data(\"CIFAR-10-test\")\n",
    "\n",
    "    # initialize a random weight matrix for CIFAR-10\n",
    "    W = np.random.rand(10, 3073)*0.001\n",
    "\n",
    "    # train the weights, epoch loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss, accuracy = SVM_loss(xTrain, yTrain, W, theta=THETA)\n",
    "        print(\n",
    "            f'In epoch {epoch}, loss is {loss}\\nIn epoch {epoch} accuracy is {accuracy}')\n",
    "\n",
    "    # eval_gradient(loss_fun,W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only used in load_train_data() and load_test_data() to read pickle file\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(filename):\n",
    "    \"\"\"\n",
    "    Load CIFAR-10 train data\n",
    "    xTrain combined image arrays\n",
    "    yTrain is the vector of correct labels for each image\n",
    "    \"\"\"\n",
    "\n",
    "    # get train data 1-6 append them to a list\n",
    "    temp_xTrain = []\n",
    "    temp_yTrain = []\n",
    "    for file in os.listdir(filename):\n",
    "        data = unpickle(os.path.join(filename, file))\n",
    "        temp_xTrain.append(data[b'data'])\n",
    "        temp_yTrain.append(data[b'labels'])\n",
    "\n",
    "    # make one list from the list of lists train data is now ready\n",
    "    xTrain = np.concatenate(temp_xTrain)\n",
    "    yTrain = np.concatenate(temp_yTrain)\n",
    "\n",
    "    # change datatype to float\n",
    "    xTrain = xTrain.astype('float64')\n",
    "\n",
    "    # normalize data ???\n",
    "    for id, im in enumerate(xTrain):\n",
    "        normIm = im/255\n",
    "        xTrain[id] = normIm\n",
    "\n",
    "    # center data ???\n",
    "    avIm = xTrain.mean(axis=0, keepdims=True)\n",
    "    for id, im in enumerate(xTrain):\n",
    "        normIm = im-avIm\n",
    "        xTrain[id] = normIm\n",
    "\n",
    "    # append bias to images arrays\n",
    "    xTrain = np.c_[xTrain, np.ones(50000).T]\n",
    "\n",
    "    # assign 1/5 of the images as validation dataset\n",
    "    xValid = xTrain[:len(xTrain)//5]\n",
    "    xTrain = xTrain[len(xTrain)//5:]\n",
    "\n",
    "    yValid = yTrain[:len(yTrain)//5]\n",
    "    yTrain = yTrain[len(yTrain)//5:]\n",
    "\n",
    "    # transpose so xTrain is 3073x40000 yTrains is 40000x1 xValid is 3073x10000 yValid is 10000x1\n",
    "    xTrain, yTrain, xValid, yValid = xTrain.T, yTrain.T, xValid.T, yValid.T\n",
    "\n",
    "    return (xTrain, yTrain, xValid, yValid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(filename):\n",
    "    \"\"\"\n",
    "    Load CIFAR-10 test data\n",
    "    \"\"\"\n",
    "    data = unpickle(os.path.join(filename, os.listdir(filename)[0]))\n",
    "    xTest = (data[b'data'])\n",
    "    yTest = (data[b'labels'])\n",
    "    yTest = np.array(yTest)\n",
    "\n",
    "    # change datatype to float\n",
    "    xTest = xTest.astype('float64')\n",
    "\n",
    "    # normalize data ???\n",
    "    for id, im in enumerate(xTest):\n",
    "        normIm = im/255\n",
    "        xTest[id] = normIm\n",
    "\n",
    "    # center data ???\n",
    "    avIm = xTest.mean(axis=0, keepdims=True)\n",
    "    for id, im in enumerate(xTest):\n",
    "        normIm = im-avIm\n",
    "        xTest[id] = normIm\n",
    "\n",
    "    # append bias to images arrays\n",
    "    xTest = np.c_[xTest, np.ones(10000).T]\n",
    "\n",
    "    # transpose so that xTest is 3073x10000 yTest is 10000\n",
    "    xTest, yTest = xTest.T, yTest.T\n",
    "    return (xTest, yTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_loss(X, y, W, theta):\n",
    "    \"\"\"\n",
    "    Multiclass Support Vector Machine Loss\n",
    "\n",
    "    x is matrix of flattened image vectors with appended biases\n",
    "    y is the integer valued matrix representing correct labels for images\n",
    "    W is the weight matrix\n",
    "    theta is the hyperparameter and usually determined by cross-validation\n",
    "\n",
    "    We set up SVM loss so that for each image,\n",
    "    its score of correct class is higher than every other\n",
    "    incorrect class by a fixed margin.\n",
    "\n",
    "    Multiclass SVM loss is formalized as:\n",
    "\n",
    "    L = data_loss + theta*regularization_penalty\n",
    "\n",
    "    wherein \n",
    "    data_loss =  (1/numOfTrainingData) *   sum    (max(0, (s_j - s_yi + margin)))\n",
    "                                         (j != yi)\n",
    "    (we assign margin value 1 and reduce number of hyperparameters\n",
    "    because theta and margin are inverse proportional)\n",
    "\n",
    "    regularization penalty = sum (sum ((W_k,l)**2))\n",
    "                             (k)   (l)    \n",
    "    \"\"\"\n",
    "    margin = 1\n",
    "\n",
    "    # weight times images\n",
    "    score_matrix = W@X\n",
    "\n",
    "    # get the correct class score for every image\n",
    "    yi_scores = score_matrix[y, np.arange(score_matrix.shape[1])]\n",
    "    yi_scores = np.matrix(yi_scores)\n",
    "\n",
    "    # margins equals to below part, to ignore when s_j == s_yi we overwrite them by assigning 0s\n",
    "    #    sum    (max(0, (s_j - s_yi + margin)))\n",
    "    # (j != yi)\n",
    "    margins = np.maximum(0, score_matrix - yi_scores + margin)\n",
    "    margins[y, np.arange(X.shape[1])] = 0\n",
    "\n",
    "    # data loss and regularization part\n",
    "    data_loss = np.mean(np.sum(margins, axis=0))\n",
    "    regularization_penalty = np.sum(np.square(W))\n",
    "\n",
    "    # total loss\n",
    "    SVM_loss_score = data_loss + theta*regularization_penalty\n",
    "\n",
    "    # find class where the score is the highest\n",
    "    yPredict = np.argmax(score_matrix, axis=0)\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = np.mean(yPredict == y)\n",
    "    \n",
    "    return (SVM_loss_score, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(W):\n",
    "    return SVM_loss(xTrain, yTrain, W, theta=THETA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_gradient(f, score_matrix, x):\n",
    "    \"\"\"\n",
    "    Analytic gradient of SVM\n",
    "\n",
    "    L = data_loss + theta*regularization_penalty\n",
    "\n",
    "    data_loss =  (1/numOfTrainingData) *   sum    (max(0, (s_j - s_yi + margin)))\n",
    "                                         (j != yi)\n",
    "\n",
    "    regularization penalty = sum (sum ((W_k,l)**2))\n",
    "                             (k)   (l)    \n",
    "\n",
    "    \"\"\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss is 9.003714240156306\n",
      "In epoch 0 accuracy is 0.08825\n",
      "In epoch 1, loss is 9.003714240156306\n",
      "In epoch 1 accuracy is 0.08825\n",
      "In epoch 2, loss is 9.003714240156306\n",
      "In epoch 2 accuracy is 0.08825\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19f8f9ebccd493d1979261b88c51ecd06bf2efdee26e5f5e5ddb3d1c8ea2e26f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
